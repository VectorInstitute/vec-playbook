defaults:
  - vllm_base

vllm_cache_dir_base: "${oc.env:SCRATCH}/.cache/vllm_compiled_graphs/"

base_vllm_config:
  &base_vllm_config # # required for vaughan or if you installed vllm in a separate venv
  submitit_python: "templates/configs/compute/vaughan/run_in_container.sh uv run python"
  submitit_args:
    gres: "gpu:l40s:1" # for killarney
    # gres: "gpu:a40:1" # for vaughan
    timeout_min:
    account: "${user.slurm.account}"

trainer:
  base_model: "/model-weights/Qwen2.5-1.5B-Instruct"
  tokenizer_name: "${base_model}"
  checkpoint_folder: "${oc.env:SCRATCH}/checkpoints/20251127-Qwen2.5-1.5B-Instruct"
  keep_all_checkpoints: false
  submitit_logs_folder: ${work_dir}
  num_epochs: 100

  rollout_vllm:
    <<: *base_vllm_config

    max_model_len: 2048
    num_replicas: 1 # 5 runners via SLURM, plus local GPU
    concurrency_per_replica: 128

    cache_dir: "${vllm_cache_dir_base}/${trainer.base_model}"

  llm_judge_vllm:
    <<: *base_vllm_config

    max_model_len: 2048
    num_replicas: 1
    concurrency_per_replica: 128

    model_name: "/model-weights/Qwen3-8B"
    cache_dir: "${vllm_cache_dir_base}/${llm_judge_vllm.model_name}"

  data:
    dataset_name: "openai/gsm8k"
    subset: "main"
    train_split: "train[:1000]" # subsample
    test_split: "test[:100]" # subsample
    query_column: "question"
    target_column: "answer"
    target_regexp: "#### (\\d+)"

  hyperparameters:
    max_model_len: 2048

    batch_size_forward: 2
    batch_size_backprop: 2
    grad_acc_steps: 8
