defaults:
  - vllm_base

vllm_cache_dir_base: "${oc.env:SCRATCH}/.cache/vllm_compiled_graphs/"

base_vllm_config: &base_vllm_config
  uv_venv: "${oc.env:SCRATCH}/uv-venvs/vllm-serving/" # optional; useful if you installed vllm in a separate venv.
  submitit_python: "templates/configs/compute/vaughan/run_in_container.sh uv run python" # required for vaughan
  submitit_args:
    partition: "a40"
    qos: "m5"
    account: "${user.slurm.account}"

trainer:
  base_model: "/model-weights/Qwen2.5-1.5B-Instruct"
  tokenizer_name: "${base_model}"
  checkpoint_folder: "${oc.env:SCRATCH}/checkpoints/20251123-Qwen2.5-1.5B-Instruct"
  keep_all_checkpoints: false
  submitit_logs_folder: ${work_dir}
  num_epochs: 100

  rollout_vllm:
    <<: *base_vllm_config

    max_model_len: 2048
    num_replicas: 5 # 5 runners via SLURM, plus local GPU
    concurrency_per_replica: 128

    cache_dir: "${vllm_cache_dir_base}/${trainer.base_model}"

  llm_judge_vllm:
    <<: *base_vllm_config

    max_model_len: 2048
    num_replicas: 0 # Use local GPU only.
    concurrency_per_replica: 128

    model_name: "/model-weights/Qwen3-8B"
    cache_dir: "${vllm_cache_dir_base}/${llm_judge_vllm.model_name}"

  data:
    dataset_name: "openai/gsm8k"
    subset: "main"
    train_split: "train[:1000]" # subsample
    test_split: "test[:100]" # subsample
    query_column: "question"
    target_column: "answer"
    target_regexp: "#### (\\d+)"

  hyperparameters:
    max_model_len: 2048

    batch_size_forward: 2
    batch_size_backprop: 2
    grad_acc_steps: 8
