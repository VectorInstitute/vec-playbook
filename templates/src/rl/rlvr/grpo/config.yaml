defaults:
  - _global # Import global config settings
  - _self_ # Must include the settings from this file

hydra:
  job:
    name: hydra_rl_rlvr_grpo
  searchpath:
    - pkg://configs # Include configs from the configs package in the searchpath

vllm_cache_dir_base: "${oc.env:SCRATCH}/.cache/vllm_compiled_graphs/"

base_vllm_config: &base_vllm_config
  # (base config for reuse across rollout and judge)
  # "submitit_python" is required for vaughan or if you installed vllm in a separate venv
  # submitit_python: "templates/configs/compute/bon_echo/run_in_container.sh uv run python"
  submitit_args:
    gres: "gpu:l40s:1" # for killarney
    # gres: "gpu:a40:1" # for vaughan
    timeout_min:
    account: "${user.slurm.account}"

trainer:
  # Model paths assume Vector cluster infrastructure where weights are pre-downloaded.
  # Override via CLI: uv run python -m rl.rlvr.grpo.launch trainer.base_model=/your/path
  base_model: "/model-weights/Qwen2.5-1.5B-Instruct"
  tokenizer_name: "${trainer.base_model}"
  checkpoint_folder: "${oc.env:SCRATCH}/checkpoints/20251127-Qwen2.5-1.5B-Instruct"
  keep_all_checkpoints: false
  submitit_logs_folder: ${paths.work_dir}
  num_epochs: 100

  run_name: "grpo"

  data:
    dataset_name: "openai/gsm8k"
    subset: "main"
    train_split: "train[:1000]" # subsample
    test_split: "test[:100]" # subsample
    query_column: "question"
    target_column: "answer"
    target_regexp: "#### (\\d+)"

  hyperparameters:
    max_model_len: 2048

    batch_size_forward: 2
    batch_size_backprop: 2
    grad_acc_steps: 8

  rollout_vllm:
    <<: *base_vllm_config

    max_model_len: 2048
    num_replicas: 1 # 5 runners via SLURM, plus local GPU
    concurrency_per_replica: 128

    cache_dir: "${vllm_cache_dir_base}/${..base_model}"

  llm_judge_vllm:
    <<: *base_vllm_config

    # Override via CLI: uv run python -m rl.rlvr.grpo.launch trainer.llm_judge_vllm.model_name=/your/path
    model_name: "/model-weights/Qwen3-8B"
    max_model_len: 2048
    num_replicas: 1
    concurrency_per_replica: 128

    cache_dir: "${vllm_cache_dir_base}/${.model_name}/"
